{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT classification heise ticker",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftsaWyMWgML8"
      },
      "source": [
        "# BERT-Klassifikation der Heise-Newsticker-Meldungen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSx16UvigML9"
      },
      "source": [
        "Kontakt: christian.winkler@datanizing.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS9wBwu7gML-"
      },
      "source": [
        "Es lohnt sich durchaus, andere Modelle von https://huggingface.co/models auszuprobierenwie z.B. `dbmdz/bert-base-german-uncased` von der Bayerischen Staatsbibliothek."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpboHpI6gML-"
      },
      "source": [
        "Angelehnt an https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC8n_sLsgML_"
      },
      "source": [
        "# Torch-Konfiguration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW9E06dqgMMA"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU %s\" % torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU :-(\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOC-fiJPgMMF"
      },
      "source": [
        "# Daten einlesen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tGKaIeYgkny"
      },
      "source": [
        "!test -f newsticker2019.db || wget https://datanizing.com/bert/heise/newsticker2019.db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4pMhLJZgMMG"
      },
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "sql = sqlite3.connect(\"newsticker2019.db\")\n",
        "df = pd.read_sql(\"SELECT text, quality FROM news WHERE quality IN ('good', 'bad')\", sql)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0hrvTB7gMMJ"
      },
      "source": [
        "# Labels auf Integer wandeln\n",
        "df[\"label\"] = 0\n",
        "df.loc[df[\"quality\"] == \"good\", \"label\"] = 1\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbmOtHSggMMN"
      },
      "source": [
        "# in Arrays wandeln\n",
        "text = df[\"text\"].values\n",
        "labels = df[\"label\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e0aURDPgMMQ"
      },
      "source": [
        "# Tokenisierung"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x67RcXgFiS_5"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyWJqjINgMMR"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqjhF_asgMMU"
      },
      "source": [
        "# Bestimmung der Maximallänge der Sätze, um Platz zu sparen\n",
        "max_len = max([len(tokenizer.encode(t, add_special_tokens=True)) for t in text])\n",
        "max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPZ7oro0gMMZ"
      },
      "source": [
        "# Jetzt alle Sätze tokenisieren und IDs merken\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for t in text:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        t,\n",
        "                        add_special_tokens = True,    # '[CLS]' und '[SEP]'\n",
        "                        max_length = 64,\n",
        "                        truncation = True,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,  # Attention-Masks erzeugen\n",
        "                        return_tensors = 'pt',         # pytorch-Tensoren als Ergebnis\n",
        "                   )\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Python-Listen in Tensoren wandeln\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Headline, Tokenisierung und IDs anzeigen\n",
        "print(text[0])\n",
        "print(tokenizer.tokenize(text[0]))\n",
        "print(input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5URLCp4gMMc"
      },
      "source": [
        "# Daten aufteilen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xQ1_RXTgMMd"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Wir arbeiten ab jetzt nur noch mit dem Input-Tensor, der Attention Mask und den Labeln\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# wir nutzen einen 3:1-Split fÃ¼r Training und Validierung\n",
        "train_size = int(0.75 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "# reproduzierbar arbeiten!\n",
        "torch.manual_seed(42)\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(train_size, val_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcxNtbSEgMMg"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# die BERT-Autoren empfehlen für Finetuning Batch-Sizes von 16 oder 32\n",
        "batch_size = 32\n",
        "\n",
        "# DataLoader fÃ¼r die beiden Datensets erzeugen (man könnte auch RandomSampler verwenden)\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
        "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alypLuKZgMMi"
      },
      "source": [
        "# Modell laden"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6WF1KWmgMMj"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# das Modell muss zum Tokenizer passen!\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-uncased\", \n",
        "    num_labels = 2, # wir haben nur gut oder shlecht\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False # wir benÃ¶tigen keine Embeddings\n",
        ")\n",
        "# hier evtl. model.cpu() einsetzen\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMBozX1PgMMl"
      },
      "source": [
        "# Optimierer auswählen, AdamW ist Standard\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMLp7uE5gMMo"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# vier Epochen, das kann justiert werden\n",
        "epochs = 4\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2n-AetfgMMr"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Accuracy berechnen\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HEOE0WYgMMu"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "# alle Zufallszahlengeneratoren initialisieren (Reproduzierbarkeit)\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Statistik fÃ¼r das Training\n",
        "training_stats = []\n",
        "\n",
        "for epoch_i in trange(epochs, desc=\"Epoche\"):\n",
        "    # akkumulierter Loss für diese Epoche\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Modell in Trainingsmodus stellen\n",
        "    model.train()\n",
        "\n",
        "    # Trainig pro Batch\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
        "        # Daten entpacken und in device-Format wandeln\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Gradienten löschen\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Vorwärts-Auswertung (Trainingsdaten vorhersagen)\n",
        "        res = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Loss berechnen und akkumulieren\n",
        "        total_train_loss += res.loss.item()\n",
        "\n",
        "        # Rückwärts-Auswertung, um Gradienten zu bestimmen\n",
        "        res.loss.backward()\n",
        "\n",
        "        # Gradient beschrÃ¤nken wegen Exploding Gradient\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Parameter und Lernrate aktualisieren\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "      \n",
        "\n",
        "    # Modell in Vorhersage-Modus umstellen\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Eine Epoche validieren\n",
        "    for batch in tqdm(validation_dataloader, desc=\"Validierung\"):\n",
        "        # jetzt die Validierungs-Daten entpacken\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Rückwärts-Auswertung wird nicht benötigt, daher auch kein Gradient\n",
        "        with torch.no_grad():        \n",
        "            # Vorhersage durchfÃ¼hren\n",
        "            res = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Loss akkumulieren\n",
        "        total_eval_loss += res.loss.item()\n",
        "\n",
        "        # Vorhersagedaten in CPU-Format wandeln, um Accuracy berechnen zu können\n",
        "        logits = res.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Accuracy für die Verifikation\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    tqdm.write(\"Accuracy: %f\" % avg_val_accuracy)\n",
        "\n",
        "    # Loss über alle Batches\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    tqdm.write(\"Validation loss %f\" % avg_val_loss)\n",
        "\n",
        "    # Statistik speichern für Auswertung\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Validierung Loss': avg_val_loss,\n",
        "            'Accuracy': avg_val_accuracy\n",
        "        }\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRtp5jhhgMMy"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_stats = pd.DataFrame(data=training_stats).set_index(\"epoch\")\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpGePApMgMM3"
      },
      "source": [
        "df_stats.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niwyInnGvtEt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}